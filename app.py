import streamlit as st
from google.analytics.data_v1beta import BetaAnalyticsDataClient
from google.analytics.data_v1beta.types import (
    RunReportRequest, DateRange, Metric, Dimension
)
from datetime import datetime
import json
import pytz
import pandas as pd
import urllib.parse
import re

# ---------------------------------------------------------
# 0. ãƒšãƒ¼ã‚¸è¨­å®š & ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰èªè¨¼
# ---------------------------------------------------------
st.set_page_config(page_title="Blog Analytics Pro", layout="wide")

def check_password():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if not st.session_state.authenticated:
        st.title("ğŸ”’ ãƒ­ã‚°ã‚¤ãƒ³")
        password_input = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")
        if st.button("ãƒ­ã‚°ã‚¤ãƒ³"):
            if password_input == st.secrets["auth"]["password"]:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé•ã„ã¾ã™")
        st.stop()

check_password()

# =========================================================
#  ãƒ¡ã‚¤ãƒ³å‡¦ç†
# =========================================================

st.title("ğŸ“Š ãƒ–ãƒ­ã‚°åˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ (Keyword Focus)")

JST = pytz.timezone('Asia/Tokyo')
now = datetime.now(JST)
current_hour = now.hour

# ---------------------------------------------------------
# 1. èªè¨¼
# ---------------------------------------------------------
try:
    creds_json = json.loads(st.secrets["gcp_service_account"])
    client = BetaAnalyticsDataClient.from_service_account_info(creds_json)
except Exception as e:
    st.error(f"GCPèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# ---------------------------------------------------------
# 2. ãƒ–ãƒ­ã‚°è¨­å®š
# ---------------------------------------------------------
BLOGS = [
    {"name": "ğŸš™ ã‚¸ãƒ ãƒ‹ãƒ¼ãƒ•ãƒªãƒ¼ã‚¯ï¼", "id": "470121869", "url": "jimm.hateblo.jp"}, 
    {"name": "ğŸ£ ã‚½ãƒ«ãƒˆãƒ«ã‚¢ãƒ¼ã®ã™ã™ã‚ï¼", "id": "343862616", "url": "sbs614.hateblo.jp"},
    {"name": "ğŸ‘” å…¬å‹™å“¡è»¢è·ãƒãƒ³", "id": "445135719", "url": "tdf.hatenablog.com"},
]

# ---------------------------------------------------------
# 3. ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ­ã‚¸ãƒƒã‚¯
# ---------------------------------------------------------

# â‘  ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ PV
def get_realtime_metrics(property_id):
    try:
        req_today = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date="today", end_date="today")],
            metrics=[Metric(name="screenPageViews")],
        )
        res_today = client.run_report(req_today)
        pv_today = int(res_today.rows[0].metric_values[0].value) if res_today.rows else 0

        req_yest = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date="yesterday", end_date="yesterday")],
            dimensions=[Dimension(name="hour")],
            metrics=[Metric(name="screenPageViews")],
        )
        res_yest = client.run_report(req_yest)
        
        pv_yest_same = 0
        pv_yest_total = 0
        if res_yest.rows:
            for row in res_yest.rows:
                h = int(row.dimension_values[0].value)
                pv = int(row.metric_values[0].value)
                pv_yest_total += pv
                if h <= current_hour:
                    pv_yest_same += pv
                    
        return pv_today, pv_yest_same, pv_yest_total
    except Exception:
        return 0, 0, 0

# â‘¡ æ—¥åˆ¥æ¨ç§»ã‚°ãƒ©ãƒ•
def get_daily_trend_comparison(property_id, days):
    current_start = f"{days}daysAgo"
    current_end = "today"
    prev_start = f"{days*2}daysAgo"
    prev_end = f"{days+1}daysAgo"

    try:
        req_curr = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date=current_start, end_date=current_end)],
            dimensions=[Dimension(name="date")],
            metrics=[Metric(name="screenPageViews")],
            order_bys=[{"dimension": {"dimension_name": "date"}}]
        )
        res_curr = client.run_report(req_curr)
        
        req_prev = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date=prev_start, end_date=prev_end)],
            dimensions=[Dimension(name="date")],
            metrics=[Metric(name="screenPageViews")],
            order_bys=[{"dimension": {"dimension_name": "date"}}]
        )
        res_prev = client.run_report(req_prev)

        curr_data = [int(row.metric_values[0].value) for row in res_curr.rows] if res_curr.rows else []
        prev_data = [int(row.metric_values[0].value) for row in res_prev.rows] if res_prev.rows else []

        min_len = min(len(curr_data), len(prev_data))
        if min_len == 0: return pd.DataFrame(), sum(curr_data), sum(prev_data)

        df = pd.DataFrame({
            "ä»ŠæœŸã®PVæ¨ç§»": curr_data[:min_len],
            "å‰æœŸã®PVæ¨ç§»": prev_data[:min_len]
        })
        
        return df, sum(curr_data), sum(prev_data)
    except Exception:
        return pd.DataFrame(), 0, 0

# â‘¢ è¨˜äº‹ãƒ©ãƒ³ã‚­ãƒ³ã‚° (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»æµå…¥å…ƒ æ··åˆå–å¾—ãƒ­ã‚¸ãƒƒã‚¯)
def get_article_ranking_comparison(property_id, days):
    current_start = f"{days}daysAgo"
    current_end = "today"
    prev_start = f"{days*2}daysAgo"
    prev_end = f"{days+1}daysAgo"

    raw_data = []

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—ã«æŒ‘æˆ¦
    try:
        req_curr = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date=current_start, end_date=current_end)],
            dimensions=[
                Dimension(name="pageTitle"), 
                Dimension(name="organicGoogleSearchQuery"), # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
                Dimension(name="sessionSourceMedium")       # æµå…¥å…ƒ
            ],
            metrics=[Metric(name="screenPageViews"), Metric(name="organicGoogleSearchAveragePosition")],
            limit=3000
        )
        res_curr = client.run_report(req_curr)
        
        if res_curr.rows:
            for row in res_curr.rows:
                title = row.dimension_values[0].value
                kw = row.dimension_values[1].value
                source = row.dimension_values[2].value
                pv = int(row.metric_values[0].value)
                rank = float(row.metric_values[1].value)

                # è¡¨ç¤ºãƒ‡ãƒ¼ã‚¿ã®å„ªå…ˆé †ä½æ±ºå®š
                # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹ãªã‚‰ãã‚Œã‚’æ¡ç”¨
                # 2. ãªã‘ã‚Œã°æµå…¥å…ƒã‚’æ¡ç”¨
                display_info = ""
                is_valid_kw = False
                
                if kw and kw not in ["(not set)", "(not provided)", ""]:
                    display_info = kw
                    is_valid_kw = True
                else:
                    display_info = f"[{source}]"
                    rank = 0

                if title and title != "(not set)":
                    raw_data.append({
                        "title": title, 
                        "info": display_info, 
                        "pv": pv, 
                        "rank": rank,
                        "is_kw": is_valid_kw
                    })

    except Exception as e:
        # æ¨©é™ä¸è¶³ãªã©ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå–ã‚Œãªã„å ´åˆã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        err_msg = str(e)
        if "organicGoogleSearchQuery is not a valid dimension" in err_msg:
            st.error(f"âš ï¸ **æ¨©é™ã‚¨ãƒ©ãƒ¼ (ID: {property_id})**")
            st.error("Google Search Consoleã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            st.info("â€» ãƒ­ãƒœãƒƒãƒˆã®ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’Search Consoleã®ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ¨©é™ã€ã«è¿½åŠ ã—ã¾ã—ãŸã‹ï¼Ÿ")
        return pd.DataFrame()

    df_curr = pd.DataFrame(raw_data)
    if df_curr.empty: return pd.DataFrame()

    # å‰æœŸãƒ‡ãƒ¼ã‚¿ (æ¯”è¼ƒç”¨)
    prev_rank_map = {}
    try:
        req_prev = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date=prev_start, end_date=prev_end)],
            dimensions=[Dimension(name="pageTitle"), Dimension(name="organicGoogleSearchQuery")],
            metrics=[Metric(name="organicGoogleSearchAveragePosition")],
            limit=3000
        )
        res_prev = client.run_report(req_prev)
        if res_prev.rows:
            for row in res_prev.rows:
                t = row.dimension_values[0].value
                k = row.dimension_values[1].value
                r = float(row.metric_values[0].value)
                prev_rank_map[(t, k)] = r
    except: pass

    # å‰æœŸPV
    prev_pv_map = {}
    try:
        req_prev_pv = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date=prev_start, end_date=prev_end)],
            dimensions=[Dimension(name="pageTitle")],
            metrics=[Metric(name="screenPageViews")],
            limit=3000
        )
        res_prev_pv = client.run_report(req_prev_pv)
        if res_prev_pv.rows:
            for row in res_prev_pv.rows:
                prev_pv_map[row.dimension_values[0].value] = int(row.metric_values[0].value)
    except: pass

    # é›†è¨ˆå‡¦ç†
    df_grouped = df_curr.groupby("title")["pv"].sum().reset_index().rename(columns={"pv": "ä»ŠæœŸã®PV"})
    df_grouped["å‰æœŸã®PV"] = df_grouped["title"].map(prev_pv_map).fillna(0).astype(int)
    
    df_grouped["å·®åˆ†"] = df_grouped["ä»ŠæœŸã®PV"] - df_grouped["å‰æœŸã®PV"]
    def calc_pct(row):
        if row["å‰æœŸã®PV"] > 0: return f"{(row['å·®åˆ†'] / row['å‰æœŸã®PV'] * 100):+.1f}%"
        elif row["ä»ŠæœŸã®PV"] > 0: return "NEW"
        else: return "0%"
    df_grouped["å‰æœŸé–“æ¯”"] = df_grouped.apply(calc_pct, axis=1)

    # è©³ç´°ã‚«ãƒ©ãƒ ã®ç”Ÿæˆ (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æµå…¥å…ƒã®æ··åœ¨)
    def format_mixed_info(title):
        rows = df_curr[df_curr["title"] == title]
        # PVãŒå¤šã„é †ã«ä¸Šä½3ã¤ã‚’è¡¨ç¤º
        top_items = rows.groupby("info")[["pv", "rank", "is_kw"]].max().sort_values("pv", ascending=False).head(3)
        res = []
        for info, row in top_items.iterrows():
            if row["is_kw"]:
                kw = info
                cr = row["rank"]
                pr = prev_rank_map.get((title, kw), 0)
                rank_str = f"{cr:.1f}ä½"
                if pr > 0:
                    diff = pr - cr
                    if diff > 0: rank_str += f"(â¬†{diff:.1f})"
                    elif diff < 0: rank_str += f"(â¬‡{abs(diff):.1f})"
                else: rank_str += "(NEW)"
                res.append(f"{kw}: {rank_str}")
            else:
                res.append(f"{info}")
        return " | ".join(res)

    df_grouped["æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ / æµå…¥å…ƒ"] = df_grouped["title"].apply(format_mixed_info)
    final = df_grouped.sort_values("ä»ŠæœŸã®PV", ascending=False).head(30)
    final = final[["title", "ä»ŠæœŸã®PV", "å‰æœŸã®PV", "å‰æœŸé–“æ¯”", "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ / æµå…¥å…ƒ"]].rename(columns={"title": "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«"})
    return final

# â‘£ SNSæµå…¥åˆ†æ
def get_sns_traffic_safe(property_id, domain, days=7):
    start_date = f"{days}daysAgo"
    try:
        request = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date=start_date, end_date="today")],
            dimensions=[Dimension(name="sessionSource"), Dimension(name="pageTitle"), Dimension(name="pagePath")],
            metrics=[Metric(name="screenPageViews")],
            limit=5000
        )
        response = client.run_report(request)
    except Exception:
        return pd.DataFrame()

    data = []
    sns_pattern = re.compile(r"t\.co|twitter|facebook|instagram|linkedin|pinterest|youtube|threads", re.IGNORECASE)
    
    if response.rows:
        for row in response.rows:
            source = row.dimension_values[0].value
            title = row.dimension_values[1].value
            path = row.dimension_values[2].value
            pv = int(row.metric_values[0].value)
            
            if sns_pattern.search(source):
                label = source
                if "t.co" in source or "twitter" in source: label = "X (Twitter)"
                elif "facebook" in source: label = "Facebook"
                elif "instagram" in source: label = "Instagram"
                elif "threads" in source: label = "Threads"
                
                full_url = f"{domain}{path}"
                search_url = f"https://search.yahoo.co.jp/realtime/search?p={urllib.parse.quote(full_url)}"
                
                data.append({"SNS": label, "è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«": title, "PV": pv, "search_link": search_url})
            
    return pd.DataFrame(data)

# â‘¤ æ¥ç¶šè¨ºæ–­ãƒ¢ãƒ¼ãƒ‰
def run_diagnostic(property_id):
    st.write("---")
    st.markdown(f"#### ğŸ•µï¸ GA4 Ã— SearchConsole æ¥ç¶šè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆ (ID: `{property_id}`)")
    
    try:
        req = RunReportRequest(
            property=f"properties/{property_id}",
            date_ranges=[DateRange(start_date="30daysAgo", end_date="today")],
            dimensions=[Dimension(name="organicGoogleSearchQuery")],
            metrics=[Metric(name="screenPageViews")],
            limit=100
        )
        res = client.run_report(req)
        
        valid_kw_sample = []
        if res.rows:
            for row in res.rows:
                kw = row.dimension_values[0].value
                if kw not in ["(not set)", "(not provided)", ""]:
                    valid_kw_sample.append(kw)
        
        if len(valid_kw_sample) > 0:
            st.success(f"âœ… **æ¥ç¶šæˆåŠŸï¼** {len(valid_kw_sample)} å€‹ã®æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            st.markdown(f"**æ¤œå‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¾‹:** `{', '.join(valid_kw_sample[:5])}`...")
        else:
            st.warning("âš ï¸ æ¥ç¶šã¯ã§ãã¦ã„ã¾ã™ãŒã€æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒ0ä»¶ã§ã™ã€‚ï¼ˆ(not set)ã®ã¿ï¼‰")
            st.info("ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã«ã‚ˆã‚‹é™¤å¤–ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            
    except Exception as e:
        err_msg = str(e)
        st.error("âŒ **æ¥ç¶šã‚¨ãƒ©ãƒ¼ã¾ãŸã¯è¨­å®šç„¡åŠ¹**")
        if "not a valid dimension" in err_msg or "organicGoogleSearchQuery" in err_msg:
            st.error(f"""
            **ã€é‡è¦ã€‘æ¨©é™è¨­å®šãŒå¿…è¦ã§ã™**
            
            ãƒ­ãƒœãƒƒãƒˆï¼ˆ`streamlit-user...`ï¼‰ãŒã€Google Search Consoleã®ãƒ‡ãƒ¼ã‚¿ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚
            Search Consoleã®ç®¡ç†ç”»é¢ã§ã€ã“ã®ãƒ­ãƒœãƒƒãƒˆã‚’ã€Œãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ã¨ã—ã¦è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
            """)
        else:
            st.error(f"APIã‚¨ãƒ©ãƒ¼è©³ç´°: {e}")

# ---------------------------------------------------------
# 4. ç”»é¢è¡¨ç¤º
# ---------------------------------------------------------
st.write(f"æœ€çµ‚æ›´æ–°: {now.strftime('%Y-%m-%d %H:%M:%S')}")

tab1, tab2, tab3, tab4 = st.tabs(["â±ï¸ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ PV", "ğŸ“ˆ æœŸé–“åˆ†æãƒ¬ãƒãƒ¼ãƒˆ", "ğŸ“± SNSã§ã®è¨€åŠãƒ»æµå…¥", "ğŸ› ï¸ æ¥ç¶šè¨ºæ–­"])

with tab1:
    cols = st.columns(3)
    for i, blog in enumerate(BLOGS):
        with cols[i]:
            st.subheader(blog["name"])
            try:
                today, yest_same, yest_total = get_realtime_metrics(blog["id"])
                diff = today - yest_same
                pct = (diff / yest_same * 100) if yest_same > 0 else 0
                st.metric("ä»Šæ—¥ã®PV", f"{today:,}", f"{diff:+,} ({pct:+.1f}%)")
                st.caption(f"æ˜¨æ—¥åŒæ™‚åˆ»: {yest_same:,} / æ˜¨æ—¥è¨ˆ: {yest_total:,}")
            except Exception:
                st.error("å–å¾—ã‚¨ãƒ©ãƒ¼")
    if st.button("æ›´æ–°", key="refresh_realtime"):
        st.rerun()

with tab2:
    st.markdown("### ğŸ“ˆ æœŸé–“æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ")
    col_sel, _ = st.columns([1, 2])
    with col_sel:
        period_days = st.selectbox("åˆ†ææœŸé–“", [7, 30], index=0, format_func=lambda x: f"éå» {x} æ—¥é–“")
    
    for blog in BLOGS:
        with st.expander(f"ğŸ“Š {blog['name']} ã®è©³ç´°åˆ†æ", expanded=True):
            try:
                # æ—¥åˆ¥æ¨ç§»
                df_trend, curr_sum, prev_sum = get_daily_trend_comparison(blog["id"], period_days)
                diff_total = curr_sum - prev_sum
                pct_total = (diff_total / prev_sum * 100) if prev_sum > 0 else 0
                st.markdown(f"#### ğŸ“… ç·PV: {curr_sum:,} ({pct_total:+.1f}%)")
                if not df_trend.empty:
                    st.line_chart(df_trend, color=["#FF4B4B", "#CCCCCC"]) 
                    st.caption("èµ¤ç·š: ä»ŠæœŸ / ã‚°ãƒ¬ãƒ¼ç·š: å‰æœŸ")
                
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°
                df_top = get_article_ranking_comparison(blog["id"], period_days)
                if not df_top.empty:
                    st.markdown("#### ğŸ† è¨˜äº‹åˆ¥ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP30")
                    st.dataframe(df_top, use_container_width=True, hide_index=True, height=600)
                else:
                    st.warning("ãƒ‡ãƒ¼ã‚¿ãªã—")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")

with tab3:
    st.markdown("### ğŸ“± SNSæµå…¥ & ã‚¨ã‚´ã‚µãƒ¼ãƒ")
    for blog in BLOGS:
        with st.expander(f"ğŸ’¬ {blog['name']}", expanded=True):
            try:
                df_sns = get_sns_traffic_safe(blog["id"], blog["url"], 7)
                if not df_sns.empty:
                    total_sns = df_sns["PV"].sum()
                    st.metric("SNSçµŒç”±ã®ç·PV (éå»7æ—¥)", f"{total_sns} PV")
                    st.bar_chart(df_sns.groupby("SNS")["PV"].sum(), color="#1DA1F2")
                    st.dataframe(
                        df_sns,
                        column_config={"search_link": st.column_config.LinkColumn("æŠ•ç¨¿ã‚’ç¢ºèª", display_text="æ¤œç´¢ ğŸ”")},
                        use_container_width=True, hide_index=True
                    )
                else:
                    st.info("SNSæµå…¥ãªã—")
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            st.markdown("---")
            q = urllib.parse.quote(blog.get("url", "")) 
            if q:
                c1, c2 = st.columns(2)
                c1.link_button("X(Twitter)åå¿œ", f"https://search.yahoo.co.jp/realtime/search?p={q}")
                c2.link_button("SNSå…¨ä½“Googleæ¤œç´¢", f"https://www.google.com/search?q=site:x.com+{q}+OR+site:facebook.com+{q}")

with tab4:
    st.markdown("### ğŸ› ï¸ æ¥ç¶šè¨ºæ–­ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼‰")
    st.write("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå–å¾—ã§ãã¦ã„ã‚‹ã‹ã€ç”Ÿã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")
    selected_blog = st.selectbox("è¨ºæ–­ã™ã‚‹ãƒ–ãƒ­ã‚°ã‚’é¸æŠ", [b["name"] for b in BLOGS])
    if st.button("è¨ºæ–­é–‹å§‹"):
        target_id = next(b["id"] for b in BLOGS if b["name"] == selected_blog)
        run_diagnostic(target_id)
